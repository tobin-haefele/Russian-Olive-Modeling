---
title: "Russian_Olive_Modeling"
author: "Tobin Haefele"
date: "2025-01-28"
output: html_document
---

```{r setup, include=FALSE}
#set working directory
setwd("~/GitHub/Russian-Olive-Modeling")

library(sf)
library(ggplot2)
library(tidyverse)
library(blockCV)
library(randomForest)
library(pdp)
library(arcgisbinding)
library(gstat)
library(ROCR)
library(mgcv)
library(corrplot)
library(gbm)
library(car)
library(summarytools)

source("data_import_test.R")

knitr::opts_chunk$set(echo = TRUE)
```

## Import data

```{r data import, warning=FALSE}
# use function to import data

# laptop version
russian_olive_df <- import_arcgis_data("data/RussianOlive/Default.gdb",
                                       
#russian_olive_df <- import_arcgis_data("data/MyProject1/MyProject1.gdb",
  "Model_data",
  method = "sf"
)

# import missoula county shapefile
msl_shape <- read_sf("data/MontanaStateBoundary_shp/MontanaCounties_shp")

```

## Cleaning data

```{r data cleaning, echo=FALSE}
# Select the Missoula shapefile
msl_shape <- msl_shape %>% filter(NAME == "MISSOULA")

# Convert Missoula shapefile to crs 4326 for conversion to sf object
msl_shape <- st_transform(msl_shape, crs = 6318)

# remove spaces from column names
colnames(russian_olive_df) <- gsub(" ", "_", colnames(russian_olive_df))

# select relevant columns
russian_olive_df <- russian_olive_df %>%
  dplyr::select(PA, X, Y, REAP, FROST_FREE_DAYS, ANNUAL_PRECIP,
                SUMMER_AVG_MAXTEMP, WINTER_AVG_MINTEMP, SOIL_PH, 
                SOIL_BULK_DENSITY, LEVEL1, LEVEL2, NEAR_WATER_150m)

# Convert to sf object for analysis
survey_sf <- st_as_sf(russian_olive_df,
  coords = c("X", "Y"),
  crs = 6318
)

# convert PA to factor for random forest model
survey_sf$PA <- as.factor(survey_sf$PA)

# convert to numeric
survey_sf <- survey_sf %>%
  mutate(NEAR_WATER_150m = as.numeric(as.character(NEAR_WATER_150m))) %>%
  mutate(NEAR_WATER_150m = replace_na(NEAR_WATER_150m, 0))

# convert all character columns to factors for proper modeling
survey_sf <- survey_sf %>%
  mutate_if(is.character, as.factor)

# Filter out any NA Soil_PH values to avoid issues in RF modeling
survey_sf <- survey_sf %>%
  filter(!is.na(SOIL_PH))

#filter out NA in Land cover
survey_sf <- survey_sf %>%
  filter(!is.na(LEVEL2) & LEVEL2 != "na" & LEVEL1 != " ")


```

## Exploring the cleaned data

```{r plot, echo=FALSE}
#brief summary of all the variables
summarytools::dfSummary(st_drop_geometry(survey_sf))

# Plot histograms for all numeric variables
numeric_vars <- russian_olive_df %>%
  dplyr::select(REAP, FROST_FREE_DAYS, ANNUAL_PRECIP, 
         SUMMER_AVG_MAXTEMP, WINTER_AVG_MINTEMP, SOIL_PH, 
         SOIL_BULK_DENSITY)

# Plot the histograms
numeric_vars_long <- gather(numeric_vars, key = "variable", value = "value")
ggplot(numeric_vars_long, aes(x = value)) +
  geom_histogram(bins = 30) +
  facet_wrap(~variable, scales = "free_x") +
  theme_minimal() +
  labs(title = "Distribution of Numeric Variables")

# Assuming 'russian_olive_df' has the PA (presence/absence) column
categorical_vars <- russian_olive_df %>%
  dplyr::select(PA, LEVEL1, LEVEL2)

# Function to create plots
plot_categorical_distribution <- function(data, title) {
  data_long <- data %>%
    gather(key = "variable", value = "value", LEVEL1, LEVEL2)
  
  ggplot(data_long, aes(x = value)) +
    geom_bar() +
    facet_wrap(~variable, scales = "free_x") +
    theme_minimal() +
    theme(axis.text.x = element_text(angle = 45, hjust = 1, size = 8)) +
    labs(title = title, x = "Category", y = "Count")
}

# 1. Combined (all points)
plot_categorical_distribution(categorical_vars, "Distribution of Categorical Variables (All Points)")

# 2. Absence only (PA == 0)
plot_categorical_distribution(
  categorical_vars %>% filter(PA == 0),
  "Distribution of Categorical Variables (Absence Points Only)"
)

# 3. Presence only (PA == 1)
plot_categorical_distribution(
  categorical_vars %>% filter(PA == 1),
  "Distribution of Categorical Variables (Presence Points Only)"
)

#view the correlation matrix
cor_matrix <- cor(numeric_vars, use = "complete.obs")
corrplot(cor_matrix, method = "number", tl.cex = .7)

#finally we can look at the data in Missoula county
ggplot(survey_sf) +
  geom_sf(aes(color = PA)) +
  theme_minimal() +
  labs(title = "Spatial Distribution of Russian Olive Observations")

```

## Prepping the data for modeling

```{r model prep, echo=FALSE}
set.seed(123)

# Create gstat object and compute sample variogram
v <- variogram(PA ~ 1, data = as(survey_sf, "Spatial"))

# Fit variogram model (Exponential is fine, could also try Sph or Gau)
v_model <- fit.variogram(v, model = vgm("Exp"))

# Plot
plot(v, v_model, main = "Empirical and Fitted Variogram")

# split data into training/testing accounting for spatial autocorrelation
sb <- cv_spatial(
  x = survey_sf,
  column = "PA",
  selection = "random",
  k = 10,
  size = 5543
)

#store folds
folds <- sb$folds_list

#create a dataframe to store the results of each model
model_metrics <- data.frame(
  Model = character(),
  AUC = numeric(),
  Accuracy = numeric(),
  stringsAsFactors = FALSE
)

```

## Model Testing

To ensure we have selected the best model for the job, I want to test several different models and compare their accuracy when predicting the presence of Russian Olive.

### Generalized Linear Model (GLM)

This model is a great baseline to use.

```{r logreg, echo=TRUE, warning=FALSE}
#setup test table for glm
glm_test_table <- survey_sf %>% 
  mutate(Pred = NA_real_)

#loop through each fold 
for (k in seq_along(folds)) {
  train_ids <- folds[[k]][[1]]
  test_ids  <- folds[[k]][[2]]

  train_data <- st_drop_geometry(survey_sf[train_ids, ])
  test_data  <- st_drop_geometry(survey_sf[test_ids, ])
  
  #drop LEVEL2 column from train and test data to avoid errors in unseen values
  train_data <- train_data %>%
    dplyr::select(-c(LEVEL2, LEVEL1))
  
  test_data <- test_data %>%
    dplyr::select(-c(LEVEL2, LEVEL1))
  
  # Fit logistic regression
  glm_model <- glm(
    PA ~ .,
    data = train_data,
    family = binomial
  )

  # Predict probabilities
  glm_test_table$Pred[test_ids] <- predict(
    glm_model,
    newdata = test_data,
    type = "response"
  )
}
```

```{r glm results}
#view summary
summary(glm_model)

#get predictions 
glm_pred <- prediction(glm_test_table$Pred, glm_test_table$PA)

#create ROC
glm_roc_curve <- performance(glm_pred, measure = "tpr", x.measure = "fpr")

#create auc
glm_auc_value <- performance(glm_pred, measure = "auc")@y.values[[1]]

# calculate accuracy
glm_accuracy <- sum(ifelse(glm_test_table$Pred > 0.5, 1, 0) == glm_test_table$PA) / nrow(glm_test_table)

model_metrics <- rbind(
  model_metrics,
  data.frame(Model = "GLM", AUC = glm_auc_value, Accuracy = glm_accuracy)
)

```

### Generalized Additive Model(GAM)

```{r GAM}
gam_test_table <- survey_sf %>% 
  mutate(Pred = NA_real_)

for (k in seq_along(folds)) {
  train_ids <- folds[[k]][[1]]
  test_ids  <- folds[[k]][[2]]

  train_data <- st_drop_geometry(survey_sf[train_ids, ])
  test_data  <- st_drop_geometry(survey_sf[test_ids, ])

  # Fit logistic regression
  gam_model <- gam(PA ~ 
                  s(FROST_FREE_DAYS) + 
                  s(ANNUAL_PRECIP) + 
                  s(SUMMER_AVG_MAXTEMP) + 
                  s(WINTER_AVG_MINTEMP) + 
                  s(SOIL_PH) + 
                  s(SOIL_BULK_DENSITY) + 
                  NEAR_WATER_150m,     
                data = train_data,
                family = binomial()
)

  # Predict probabilities
  gam_test_table$Pred[test_ids] <- predict(
    gam_model,
    newdata = test_data,
    type = "response"
  )
}

summary(gam_model)
```

### Random Forest Modeling

```{r model, echo=FALSE, warning=FALSE}
# Train and validate a model using the folds
test_table <- survey_sf %>% 
  mutate(Pred = NA_real_)

#loop over each fold
for (k in seq_len(length(folds))) {
  
  #select the IDs from each fold
  train_ids <- folds[[k]][[1]]
  test_ids  <- folds[[k]][[2]]
  
  #index the dataframe and assign the respective ids to sets
  train_data <- st_drop_geometry(survey_sf[train_ids, ])
  test_data  <- st_drop_geometry(survey_sf[test_ids, ])
  
  #drop landcover variables
  train_data <- train_data %>%
    dplyr::select(-c(LEVEL2, LEVEL1))
  
  test_data <- test_data %>%
    dplyr::select(-c(LEVEL2, LEVEL1))

  # Train a random forest model with training data (drop geometry)
  rf_model <- randomForest(
    PA ~ ., 
    data = train_data,
    ntree = 500,
    importance = TRUE
  )

  # Predict probabilities for the test set and store
  test_table$Pred[test_ids] <- predict(
    rf_model,
    test_data,
    type = "prob"
  )[, 2]
}
```

```{r rf results}
#summary
rf_model$importanceSD

#get predictions
rf_pred <- prediction(test_table$Pred, test_table$PA)

#create ROC
roc_curve <- performance(rf_pred, measure = "tpr", x.measure = "fpr")

#create auc
rf_auc_value <- performance(rf_pred, measure = "auc")@y.values[[1]]

# calculate accuracy
rf_accuracy <- sum(ifelse(test_table$Pred > 0.5, 1, 0) == test_table$PA) / nrow(test_table)

# add to model metrics
model_metrics <- rbind(
  model_metrics,
  data.frame(Model = "Random Forest", AUC = rf_auc_value, Accuracy = rf_accuracy)
)

```

### Gradient Boosting Machine (GBM)

```{r GBM model}
gbm_preds <- survey_sf %>%
  st_drop_geometry() %>%
  mutate(GBM_Pred = NA_real_)

for (k in seq_along(folds)) {
  train_ids <- unlist(folds[[k]][1])
  test_ids  <- unlist(folds[[k]][2])
  
  train_data <- survey_sf[train_ids, ] %>% st_drop_geometry()
  test_data  <- survey_sf[test_ids, ] %>% st_drop_geometry()
  
  train_data$PA <- as.integer(as.character(train_data$PA))
  test_data$PA  <- as.integer(as.character(test_data$PA))
  
  train_data <- train_data %>%
    dplyr::select(-c(LEVEL2, LEVEL1))
  
  test_data <- test_data %>%
    dplyr::select(-c(LEVEL2, LEVEL1))
  
  # Fit the GBM model
  model <- gbm(
    formula = PA ~ .,
    data = train_data,
    distribution = "bernoulli",
    interaction.depth = 5,
    shrinkage = 0.01,
    bag.fraction = 0.5,
    train.fraction = 1.0,
    verbose = FALSE,
    cv.folds = 5,
    n.trees = 1000
  )
  
  # Use early stopping with OOB estimate
  best_iter <- gbm.perf(model, method = "cv")

  best_iter
  
  # Predict on test data
  preds <- predict(model, newdata = test_data, n.trees = best_iter, type = "response")
  
  gbm_preds$GBM_Pred[test_ids] <- preds
}
```

```{r gbm results}
#summary
summary(model)

#get predictions
gbm_pred <- prediction(gbm_preds$GBM_Pred, gbm_preds$PA)

#create ROC
roc_curve <- performance(gbm_pred, measure = "tpr", x.measure = "fpr")

#create auc
gbm_auc_value <- performance(gbm_pred, measure = "auc")@y.values[[1]]

# calculate accuracy
gbm_accuracy <- sum(ifelse(gbm_preds$GBM_Pred > 0.5, 1, 0) == gbm_preds$PA) / nrow(gbm_preds)

# add to model metrics
model_metrics <- rbind(
  model_metrics,
  data.frame(Model = "GBM", AUC = gbm_auc_value, Accuracy = gbm_accuracy)
)

summary(model)
```

```{r results}
# Print the model metrics
print(model_metrics)


```

```{r data output}
# convert sf back to a df, keeping coordinates for plotting
predictions_df <- as.data.frame(test_table) %>%
  mutate(
    lon = sf::st_coordinates(geometry)[, 1],
    lat = sf::st_coordinates(geometry)[, 2]
  )

# remove geometry after converting to coordinates
predictions_df <- st_drop_geometry(predictions_df)

# Add a hsuitability column based on Pred values
predictions_df <- predictions_df %>%
  mutate(hsuitability = case_when(
    Pred < 0.20 ~ "Low",
    Pred >= 0.20 & Pred <= 0.70 ~ "Medium",
    Pred > 0.70 ~ "High"
  ))

```

### Potential Variables to include

-   [x] Wetland Riparian (land cover )
-   [x] Maximum Summer Temp (climate)
-   [x] Frost Free Days (climate)
-   [x] Anthropogenic Influence (land cover)
-   [x] Introduced Vegetation (land cover)
-   [x] Soil pH (soil)
-   [x] Forest - Conifer (land cover)
-   [x] Bulk Density (soil)
-   [x] Degree Days (climate)
-   [x] Distance to Water Edge (hydrography)?

### To Do

-   [ ] Visualization of current spread (potentially up against land ownership?)
-   [x] add above variables to model
-   [x] clean MT state data to under 800 spatial precision
-   [ ] some kind of polygonal/area analysis
-   [x] Create outline for paper
-   [x] Write paper
-   [ ] Try different modeling methods
-   [ ] Lay out story map

### Data sets

<https://geoinfo.msl.mt.gov/msdi/land_use_land_cover> <https://geoinfo.msl.mt.gov/msdi/climate/> <https://prism.oregonstate.edu/> <https://doi.org/10.18113/S1KW2H> (soil)
