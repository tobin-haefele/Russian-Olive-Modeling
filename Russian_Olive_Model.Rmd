---
title: "Russian_Olive_Modeling"
author: "Tobin Haefele"
date: "2025-01-28"
output: html_document
---

```{r setup, include=FALSE}
library(sf)
library(ggplot2)
library(tidyverse)
library(blockCV)
library(randomForest)
library(pdp)
library(arcgisbinding)
library(gstat)
library(ROCR)


knitr::opts_chunk$set(echo = TRUE)
```

## Import data

```{r data import, warning=FALSE}
#import path to ArcGIS project
path <- file.path("C:/Users/thaef/OneDrive/Documents/School/Russian-Olive-Modeling/MyProject1/MyProject1/MyProject1.gdb")

#get product info
arc.check_product()

# Verify the path
if (file.exists(path)) {
  # Open the geodatabase
  missoula_gdb <- arc.open(path)
  print("Geodatabase opened successfully!")
} else {
  print("Invalid path or geodatabase does not exist.")
}

# Import the table from the geodatabase
russian_olive_df <- sf::st_read(dsn = path, layer = "CombinedTable")

# import missoula county shapefile
msl_shape <- read_sf("data/MontanaStateBoundary_shp/MontanaCounties_shp")
```

## Cleaning data

```{r data cleaning, echo=FALSE}
# Select the Missoula shapefile
msl_shape <- msl_shape %>% filter(NAME == "MISSOULA")

# Convert Missoula shapefile to crs 4326 for conversion to sf object
msl_shape <- st_transform(msl_shape, crs = 4326)

# remove spaces from column names
colnames(russian_olive_df) <- gsub(" ", "_", colnames(russian_olive_df))

# select relevant columns (PA = presence/absence, X and Y = coordinates, and environmental variables)
russian_olive_df <- russian_olive_df %>%
  select(PA, X, Y, REAP_cm, FFD, soil_bulk_dens, soil_pH,
         annual_precip_mm, jan_mean_K, jul_mean_K, LEVEL1, LEVEL2)

# Convert to sf object for analysis
survey_sf <- st_as_sf(russian_olive_df,
  coords = c("X", "Y"),
  crs = 4326
)

# convert PA to factor for random forest model
survey_sf$PA <- as.factor(survey_sf$PA)

# convert all character columns to factors for proper modeling
survey_sf <- survey_sf %>%
  mutate_if(is.character, as.factor)

#convert any NA values to 0 for modeling
survey_sf[is.na(survey_sf)] <- 0
```

## Testing cleaned data

```{r plot, echo=FALSE}
# plot imported points to test
ggplot() +
  geom_sf(data = msl_shape) +
  geom_sf(data = survey_sf, aes(color = PA))
```

## Prepping the data for modeling

```{r model prep, echo=FALSE}
#set seed for reproducibility
set.seed(1234)


# split data into training/testing accounting for spatial autocorrelation
sb <- cv_spatial(
  x = survey_sf,
  column = "PA",
  selection = "random",
  size = 9000,
  k = 10
)

# store the folds for later use
folds <- sb$folds_list


```

## Basic modeling

```{r model, echo=FALSE, warning=FALSE}
# Train and validate a model using the folds
test_table <- survey_sf
test_table$Pred <- as.numeric("NA")

for (k in seq_len(length(folds))) {
  # Extracting the training and testing indices
  train_set <- unlist(folds[[k]][1]) # training set indices; first element
  test_set <- unlist(folds[[k]][2]) # testing set indices; second element

  # Train a random forest model
  rf <- randomForest(PA ~ ., st_drop_geometry(survey_sf[train_set, ]),
    ntree = 500,
    importance = TRUE
  )

  # Predict the test set and store probabilities
  test_table$Pred[test_set] <- predict(rf,
    st_drop_geometry(survey_sf[test_set, ]),
    type = "prob"
  )[, 2]
}
```

```{r results}
# view the variable importance scores
importance_scores <- importance(rf)
print(importance_scores)
varImpPlot(rf)

#print rf
print(rf)

#predictions
pred <- prediction(test_table$Pred, test_table$PA)

#roc curve
roc_curve <- performance(pred, measure="tpr", x.measure = "fpr")

#plot
plot(roc_curve, main = "ROC Curve", col = "steelblue", lwd = 2)
abline(a = 0, b = 1, lty = 2, col = "red")

auc_value <- performance(pred, measure = "auc")@y.values[[1]]
print(paste("AUC:", auc_value))


## Variable Importance
# pdp plot of each factors importance
pd <- pdp::partial(rf, pred.var = "LEVEL2", plot = FALSE)

print(pd)

## Result by point

# plot the results
ggplot() +
  geom_sf(data = msl_shape) +
  geom_sf(data = test_table, aes(color = Pred))
```

```{r data output}
#convert sf back to a df, keeping coordinates for plotting
predictions_df <- as.data.frame(test_table) %>%
  mutate(lon = sf::st_coordinates(geometry)[,1],
         lat = sf::st_coordinates(geometry)[,2])

#remove geometry after converting to coordinates
predictions_df <- st_drop_geometry(predictions_df)

# Add a hsuitability column based on Pred values
predictions_df <- predictions_df %>%
  mutate(hsuitability = case_when(
    Pred < 0.20 ~ "Low",
    Pred >= 0.20 & Pred <= 0.70 ~ "Medium",
    Pred > 0.70 ~ "High"
  ))

```

### Potential Variables to include

-   [x] Wetland Riparian (land cover )
-   [x] Maximum Summer Temp (climate)
-   [x] Frost Free Days (climate)
-   [x] Anthropogenic Influence (land cover)
-   [x] Introduced Vegetation (land cover)
-   [x] Soil pH (soil)
-   [x] Forest - Conifer (land cover)
-   [x] Bulk Density (soil)
-   [x] Degree Days (climate)
-   [x] Distance to Water Edge (hydrography)?

### To Do

-   [ ]  Visualization of current spread (potentially up against land ownership?)
-   [x]  add above variables to model
-   [x]  clean MT state data to under 800 spatial precision
-   [ ]  some kind of polygonal/area analysis
-   [x]  Create outline for paper
-   [x]  Write paper
-   [ ]  Try different modeling methods
-   [ ] Lay out story map

### Data sets

<https://geoinfo.msl.mt.gov/msdi/land_use_land_cover> <https://geoinfo.msl.mt.gov/msdi/climate/> <https://prism.oregonstate.edu/> <https://doi.org/10.18113/S1KW2H> (soil)
