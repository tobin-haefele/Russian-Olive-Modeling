---
title: "Russian_Olive_Modeling"
author: "Tobin Haefele"
date: "2025-01-28"
output: html_document
---

```{r setup, include=FALSE}
library(sf)
library(ggplot2)
library(tidyverse)
library(blockCV)
library(randomForest)
library(pdp)
library(arcgisbinding)
library(gstat)
library(ROCR)
library(maxnet)



knitr::opts_chunk$set(echo = TRUE)
```

## Import data

```{r data import, warning=FALSE}
# use function to import data
russian_olive_df <- import_arcgis_data("data/MyProject1/MyProject1.gdb",
  "Model_data",
  method = "sf"
)

# import missoula county shapefile
msl_shape <- read_sf("data/MontanaStateBoundary_shp/MontanaCounties_shp")
```

## Cleaning data

```{r data cleaning, echo=FALSE}
# Select the Missoula shapefile
msl_shape <- msl_shape %>% filter(NAME == "MISSOULA")

# Convert Missoula shapefile to crs 4326 for conversion to sf object
msl_shape <- st_transform(msl_shape, crs = 4326)

# remove spaces from column names
colnames(russian_olive_df) <- gsub(" ", "_", colnames(russian_olive_df))

# select relevant columns
russian_olive_df <- russian_olive_df %>%
  dplyr::select(PA, X, Y, REAP, FROST_FREE_DAYS, ANNUAL_PRECIP,
                SUMMER_AVG_MAXTEMP, WINTER_AVG_MINTEMP, SOIL_PH, 
                SOIL_BULK_DENSITY, LEVEL1, LEVEL2, NEAR_WATER_150m)

# Convert to sf object for analysis
survey_sf <- st_as_sf(russian_olive_df,
  coords = c("X", "Y"),
  crs = 4326
)

# convert PA to factor for random forest model
survey_sf$PA <- as.factor(survey_sf$PA)

# convert to character and replace NA with 0 for factor
survey_sf <- survey_sf %>%
  mutate(NEAR_WATER_150m = as.numeric(as.character(NEAR_WATER_150m))) %>%
  mutate(NEAR_WATER_150m = replace_na(NEAR_WATER_150m, 0))

# convert all character columns to factors for proper modeling
survey_sf <- survey_sf %>%
  mutate_if(is.character, as.factor)

# Filter out any NA Soil_PH values to avoid issues in RF modeling
survey_sf <- survey_sf %>%
  filter(!is.na(SOIL_PH))
```

## Testing cleaned data

```{r plot, echo=FALSE}
# plot imported points to test
ggplot() +
  geom_sf(data = msl_shape) +
  geom_sf(data = survey_sf, aes(color = PA))
```

## Prepping the data for modeling

```{r model prep, echo=FALSE}
# set seed for reproducibility
set.seed(1234)

# create variogram to account for spatial autocorrelation
v <- variogram(PA ~ 1, data = survey_sf)

# fit variogram model
v_model <- v %>% 
  fit.variogram(model = vgm("Exp"))

# plot variogram
plot(v, v_model)

# split data into training/testing accounting for spatial autocorrelation
sb <- cv_spatial(
  x = survey_sf,
  column = "PA",
  selection = "random",
  size = 7500,
  k = 10
)

# store the folds for later use
folds <- sb$folds_list
```

## Basic modeling

```{r model, echo=FALSE, warning=FALSE}
# Train and validate a model using the folds
test_table <- survey_sf %>% 
  mutate(Pred = NA_real_)

#loop over each fold
for (k in seq_len(length(folds))) {
  # Extracting the training and testing indices
  train_set <- unlist(folds[[k]][1]) # training set indices; first element
  test_set <- unlist(folds[[k]][2]) # testing set indices; second element

  # Train a random forest model with training data (drop geometry)
  rf_model <- randomForest(
    PA ~ ., 
    data = st_drop_geometry(survey_sf[train_set, ]),
    ntree = 500,
    importance = TRUE
  )

  # Predict probabilities for the test set and store
  test_table$Pred[test_set] <- predict(
    rf_model,
    st_drop_geometry(survey_sf[test_set, ]),
    type = "prob"
  )[, 2]
}
```

```{r results}
# view the variable importance scores
importance_scores <- importance(rf_model)
print(importance_scores)
varImpPlot(rf_model)

# print rf
print(rf_model)

# predictions
pred <- prediction(test_table$Pred, test_table$PA)

# roc curve
roc_curve <- performance(pred, measure = "tpr", x.measure = "fpr")

# plot
plot(roc_curve, main = "ROC Curve", col = "steelblue", lwd = 2)
abline(a = 0, b = 1, lty = 2, col = "red")

auc_value <- performance(pred, measure = "auc")@y.values[[1]]
print(paste("AUC:", auc_value))


## Variable Importance
# pdp plot of each factors importance
pd <- pdp::partial(rf, pred.var = "LEVEL2", plot = FALSE)

print(pd)

## Result by point

# plot the results
ggplot() +
  geom_sf(data = msl_shape) +
  geom_sf(data = test_table, aes(color = Pred))
```

```{r data output}
# convert sf back to a df, keeping coordinates for plotting
predictions_df <- as.data.frame(test_table) %>%
  mutate(
    lon = sf::st_coordinates(geometry)[, 1],
    lat = sf::st_coordinates(geometry)[, 2]
  )

# remove geometry after converting to coordinates
predictions_df <- st_drop_geometry(predictions_df)

# Add a hsuitability column based on Pred values
predictions_df <- predictions_df %>%
  mutate(hsuitability = case_when(
    Pred < 0.20 ~ "Low",
    Pred >= 0.20 & Pred <= 0.70 ~ "Medium",
    Pred > 0.70 ~ "High"
  ))
```

### Potential Variables to include

-   [x] Wetland Riparian (land cover )
-   [x] Maximum Summer Temp (climate)
-   [x] Frost Free Days (climate)
-   [x] Anthropogenic Influence (land cover)
-   [x] Introduced Vegetation (land cover)
-   [x] Soil pH (soil)
-   [x] Forest - Conifer (land cover)
-   [x] Bulk Density (soil)
-   [x] Degree Days (climate)
-   [x] Distance to Water Edge (hydrography)?

### To Do

-   [ ]  Visualization of current spread (potentially up against land ownership?)
-   [x]  add above variables to model
-   [x]  clean MT state data to under 800 spatial precision
-   [ ]  some kind of polygonal/area analysis
-   [x]  Create outline for paper
-   [x]  Write paper
-   [ ]  Try different modeling methods
-   [ ] Lay out story map

### Data sets

<https://geoinfo.msl.mt.gov/msdi/land_use_land_cover> <https://geoinfo.msl.mt.gov/msdi/climate/> <https://prism.oregonstate.edu/> <https://doi.org/10.18113/S1KW2H> (soil)
