---
title: "Russian_Olive_Modeling"
author: "Tobin Haefele"
date: "2025-01-28"
output: html_document
---

```{r setup, include=FALSE}
library(sf)
library(ggplot2)
library(tidyverse)
library(blockCV)
library(randomForest)
library(pdp)
library(arcgisbinding)
library(gstat)
library(ROCR)


knitr::opts_chunk$set(echo = TRUE)
```

## Import data

```{r data import, warning=FALSE}
# use function to import data
russian_olive_df <- import_arcgis_data("data/MyProject1/MyProject1.gdb",
  "Model_data",
  method = "sf"
)

# import missoula county shapefile
msl_shape <- read_sf("data/MontanaStateBoundary_shp/MontanaCounties_shp")

```

## Cleaning data

```{r data cleaning, echo=FALSE}
# Select the Missoula shapefile
msl_shape <- msl_shape %>% filter(NAME == "MISSOULA")

# Convert Missoula shapefile to crs 4326 for conversion to sf object
msl_shape <- st_transform(msl_shape, crs = 4326)

# remove spaces from column names
colnames(russian_olive_df) <- gsub(" ", "_", colnames(russian_olive_df))

russian_olive_df <- russian_olive_df %>%
  filter(!(Woody_setting %in% c("Ornamental", "Windbreak")) | is.na(Woody_setting))


# select relevant columns
russian_olive_df <- russian_olive_df %>%
  dplyr::select(PA, X, Y, REAP, FROST_FREE_DAYS, ANNUAL_PRECIP,
                SUMMER_AVG_MAXTEMP, WINTER_AVG_MINTEMP, SOIL_PH, 
                SOIL_BULK_DENSITY, LEVEL1, LEVEL2, NEAR_WATER_150m)

# Convert to sf object for analysis
survey_sf <- st_as_sf(russian_olive_df,
  coords = c("X", "Y"),
  crs = 4326
)

# convert PA to factor for random forest model
survey_sf$PA <- as.factor(survey_sf$PA)

# convert to character and replace NA with 0 for factor
survey_sf <- survey_sf %>%
  mutate(NEAR_WATER_150m = as.numeric(as.character(NEAR_WATER_150m))) %>%
  mutate(NEAR_WATER_150m = replace_na(NEAR_WATER_150m, 0))

# convert all character columns to factors for proper modeling
survey_sf <- survey_sf %>%
  mutate_if(is.character, as.factor)

# Filter out any NA Soil_PH values to avoid issues in RF modeling
survey_sf <- survey_sf %>%
  filter(!is.na(SOIL_PH))


```

## Testing cleaned data

```{r plot, echo=FALSE}
# plot imported points to test
ggplot() +
  geom_sf(data = msl_shape) +
  geom_sf(data = survey_sf, aes(color = PA))
```

## Prepping the data for modeling

```{r model prep, echo=FALSE}

survey_sf <- st_transform(survey_sf, crs = 32612) 

# 2. Create gstat object and compute sample variogram
v <- variogram(PA ~ 1, data = as(survey_sf, "Spatial"))

# 3. Fit variogram model (Exponential is fine, could also try Sph or Gau)
v_model <- fit.variogram(v, model = vgm("Sph"))

# 4. Plot
plot(v, v_model, main = "Empirical and Fitted Variogram")

# split data into training/testing accounting for spatial autocorrelation
sb <- cv_spatial(
  x = survey_sf,
  column = "PA",
  selection = "random",
  k = 10
)

# store the folds for later use
folds <- sb$folds_list

# Save the plot to file
ggsave("spatial_folds.png", plot = fold_plot)


```

## Initial Analysis

```{r logreg, echo=FALSE, warning=FALSE}

glm_test_table <- survey_sf %>% 
  mutate(Pred = NA_real_)

for (k in seq_along(folds)) {
  train_ids <- unlist(folds[[k]][1])
  test_ids  <- unlist(folds[[k]][2])

  train_data <- st_drop_geometry(survey_sf[train_ids, ])
  test_data  <- st_drop_geometry(survey_sf[test_ids, ])

  # Fit logistic regression
  logreg_model <- glm(
    PA ~ ., 
    data = train_data,
    family = binomial
  )

  for (col in names(train_data)) {
  if (is.factor(train_data[[col]]) && col %in% names(test_data)) {
    test_data[[col]] <- factor(test_data[[col]], levels = levels(train_data[[col]]))
    }
  }
  
  # Predict probabilities
  glm_test_table$Pred[test_set] <- predict(
    logreg_model,
    newdata = test_data,
    type = "response"
  )
}

```

## Random Forest Modeling

```{r model, echo=FALSE, warning=FALSE}
# Train and validate a model using the folds
test_table <- survey_sf %>% 
  mutate(Pred = NA_real_)

#loop over each fold
for (k in seq_len(length(folds))) {
  # Extracting the training and testing indices
  train_set <- unlist(folds[[k]][1]) # training set indices; first element
  test_set <- unlist(folds[[k]][2]) # testing set indices; second element


  # Train a random forest model with training data (drop geometry)
  rf_model <- randomForest(
    PA ~ ., 
    data = st_drop_geometry(survey_sf[train_set, ]),
    ntree = 500,
    importance = TRUE
  )

  # Predict probabilities for the test set and store
  test_table$Pred[test_set] <- predict(
    rf_model,
    st_drop_geometry(survey_sf[test_set, ]),
    type = "prob"
  )[, 2]
}
```

```{r results}
# view the variable importance scores
importance_scores <- importance(rf_model)
print(importance_scores)
varImpPlot(rf_model)

# print rf
print(rf_model)

# predictions
pred <- prediction(test_table$Pred, test_table$PA)

# roc curve
roc_curve <- performance(pred, measure = "tpr", x.measure = "fpr")

# plot
plot(roc_curve, main = "ROC Curve", col = "steelblue", lwd = 2)
abline(a = 0, b = 1, lty = 2, col = "red")

auc_value <- performance(pred, measure = "auc")@y.values[[1]]
print(paste("AUC:", auc_value))


## Variable Importance
# pdp plot of each factors importance
pd <- pdp::partial(rf, pred.var = "LEVEL2", plot = FALSE)

pdp::plotPartial(pd, )

## Result by point

# plot the results
ggplot() +
  geom_sf(data = msl_shape) +
  geom_sf(data = test_table, aes(color = Pred))
```

```{r data output}
# convert sf back to a df, keeping coordinates for plotting
predictions_df <- as.data.frame(test_table) %>%
  mutate(
    lon = sf::st_coordinates(geometry)[, 1],
    lat = sf::st_coordinates(geometry)[, 2]
  )

# remove geometry after converting to coordinates
predictions_df <- st_drop_geometry(predictions_df)

# Add a hsuitability column based on Pred values
predictions_df <- predictions_df %>%
  mutate(hsuitability = case_when(
    Pred < 0.20 ~ "Low",
    Pred >= 0.20 & Pred <= 0.70 ~ "Medium",
    Pred > 0.70 ~ "High"
  ))

#generate z-score
pred_mean <- mean(predictions_df$Pred)

pred_sd <- sd(predictions_df$Pred)

z_score <- (predictions_df$Pred - pred_mean) / pred_sd

hist(z_score)

predictions_df <- predictions_df %>%
  mutate(z_score = z_score)


```

### Potential Variables to include

-   [x] Wetland Riparian (land cover )
-   [x] Maximum Summer Temp (climate)
-   [x] Frost Free Days (climate)
-   [x] Anthropogenic Influence (land cover)
-   [x] Introduced Vegetation (land cover)
-   [x] Soil pH (soil)
-   [x] Forest - Conifer (land cover)
-   [x] Bulk Density (soil)
-   [x] Degree Days (climate)
-   [x] Distance to Water Edge (hydrography)?

### To Do

-   [ ] Visualization of current spread (potentially up against land ownership?)
-   [x] add above variables to model
-   [x] clean MT state data to under 800 spatial precision
-   [ ] some kind of polygonal/area analysis
-   [x] Create outline for paper
-   [x] Write paper
-   [ ] Try different modeling methods
-   [ ] Lay out story map

### Data sets

<https://geoinfo.msl.mt.gov/msdi/land_use_land_cover> <https://geoinfo.msl.mt.gov/msdi/climate/> <https://prism.oregonstate.edu/> <https://doi.org/10.18113/S1KW2H> (soil)
